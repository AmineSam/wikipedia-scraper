{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rru15J6cT0aF"
      },
      "source": [
        "# Your first scraper\n",
        "In this project, we will guide you step by step through the process of:\n",
        "\n",
        "1. creating a self-contained development environment.\n",
        "1. retrieving some information from an API (a website for computers)\n",
        "2. leveraging it to scrape a website that does not provide an API\n",
        "3. saving the output for later processing\n",
        "\n",
        "Here we query an API for a list of countries and their past leaders. We then extract and sanitize their short bio from Wikipedia. Finally, we save the data to disk.\n",
        "\n",
        "This task is often the first (coding) step of a datascience project and you will often come back to it in the future.\n",
        "\n",
        "You will study topics such as *scraping*, *data structures*, *regular expressions*, *concurrency* and *file handling*. We will point out useful resources at the appropriate time. \n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Creating a clean environment\n",
        "\n",
        "Use the [`venv`](https://docs.python.org/3/library/venv.html) command to create a new environment called `wikipedia_scraper_env`.\n",
        "\n",
        "Activate it and add it to you `.gitignore` file. \n",
        "\n",
        "You will find more info about virtual environments in the course content and on the web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qvKH56a1Wy"
      },
      "source": [
        "## 1. API Scraping\n",
        "\n",
        "### 1a. A simple API query\n",
        "You will start with the basics: how to do a simple request to an [API endpoint](../../2.python/2.python_advanced/05.Scraping/5.apis.ipynb).\n",
        "\n",
        "You will use the [requests](https://requests.readthedocs.io/en/latest/) external library through the `import` keyword. NOTE: external libraries need to be installed first. Check the [request Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/) section of the documentation to:\n",
        "\n",
        "1. Use the `get()` method to connect to this endpoint: https://country-leaders.onrender.com/status\n",
        "2. Check if the `status_code` is equal to 200, which means OK.\n",
        "    * if OK, `print()` the `text`` of the response.\n",
        "    * if not, `print()` the `status_code`. \n",
        "\n",
        "Here is an explanation of [HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9baoMWgIcK3E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Alive\"\n"
          ]
        }
      ],
      "source": [
        "# import the requests library (1 line)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "\n",
        "# assign the root url (without /status) to the root_url variable for ease of reference\n",
        "root_url = \"https://country-leaders.onrender.com\"\n",
        "\n",
        "# assign the /status endpoint to another variable called status_url\n",
        "status_url = f\"{root_url}/status\"\n",
        "\n",
        "# query the /status endpoint using the get() method and store it in the req variable\n",
        "req = requests.get(status_url)\n",
        "\n",
        "# check the status_code using a condition and print appropriate messages\n",
        "if req.status_code == 200:\n",
        "    print(req.text)\n",
        "else:\n",
        "    print(req.status_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL72ovJUcZTA"
      },
      "source": [
        "### 1b. Dealing with JSON\n",
        "\n",
        "[JSON](https://quickref.me/json) is the preferred format to deal with data over the web. You cannot avoid it so you would better get acquainted.\n",
        "\n",
        "Connect to another endpoint called `/countries` but this time the API will return data in the JSON format. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y0DLiYCWcg5W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "403 {'message': 'The cookie is missing'}\n"
          ]
        }
      ],
      "source": [
        "# Set the countries_url variable\n",
        "countries_url = f\"{root_url}/countries\"\n",
        "\n",
        "# query the /countries endpoint using the get() method and store it in the req variable\n",
        "req = requests.get(countries_url)\n",
        "\n",
        "# Get the JSON content and store it in the countries variable\n",
        "countries = req.json()\n",
        "\n",
        "# display the request's status code and the countries variable\n",
        "print(req.status_code, countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x25JA6vaRBi"
      },
      "source": [
        "### 1c. Cookies anyone?\n",
        "\n",
        "It looks like the access to this API is restricted...\n",
        "Query the `/cookie` endpoint and extract the appropriate field to access your cookie.\n",
        "\n",
        "You will need to use this cookie in each of the following API requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dTDwpN9Q3nk_"
      },
      "outputs": [],
      "source": [
        "# Set the cookie_url variable\n",
        "cookie_url = f\"{root_url}/cookie\"\n",
        "\n",
        "# Query the endpoint, set the cookies variable and display it\n",
        "response = requests.get(cookie_url)\n",
        "cookies = response.cookies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHBNaFJo2M9e"
      },
      "source": [
        "Try to query the countries endpoint using the cookie, save the output and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9Y63sTXY7ppT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ma', 'fr', 'us', 'ru', 'be']\n"
          ]
        }
      ],
      "source": [
        "# query the /countries endpoint, assign the output to the countries variable\n",
        "countries = requests.get(countries_url, cookies=cookies).json()\n",
        "\n",
        "# display the countries variable\n",
        "print(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3HmuGTT9lU1"
      },
      "source": [
        "Chances are the cookie has expired... Thanksfully, you got a nice error message. For now, simply execute the last 2 cells quickly so you get a result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egv40GBV8rSH"
      },
      "source": [
        "### 1d. Getting the actual data from the API\n",
        "\n",
        "Query the `/leaders` endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZwLFqcBA8PaD"
      },
      "outputs": [],
      "source": [
        "# Set the leaders_url variable\n",
        "leaders_url = f\"{root_url}/leaders\"\n",
        "\n",
        "# Choose one country and store it in the country variable\n",
        "country = countries[0]\n",
        "\n",
        "# Query the /leaders endpoint for the chosen country using the cookie\n",
        "response = requests.get(f\"{leaders_url}?country={country}\", cookies=cookies)\n",
        "leaders = response.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QyM7vWBAlY4"
      },
      "source": [
        "It looks like this endpoint requires additional information in order to return its result. Check the API [*documentation*](https://country-leaders.onrender.com/docs) in your web browser.\n",
        "\n",
        "Change the query to accept *parameters*. You should know where to find help by now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gIEFBhBeAkzf"
      },
      "outputs": [],
      "source": [
        "# query the /leaders endpoint using cookies and parameters (take any country in countries)\n",
        "# assign the output to the leaders variable\n",
        "leaders = requests.get(leaders_url, cookies=cookies, params={\"country\": countries[4]}).json()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uW3k5uquCirA"
      },
      "source": [
        "### 1e. A sneak peak at the data (finally)\n",
        "\n",
        "Look inside a few examples. Notice the dictionary keys available for each entry. You have your first example of *structured data*. This data was sanitized for your benefit, meaning it is readily exploitable without modification.\n",
        "\n",
        "You will also notice there is a Wikipedia link for each entry. You will need to extract additional information there. This will be a case of *semi-structured* data.\n",
        "\n",
        "The /countries endpoint returns a `list` of several country codes.\n",
        "\n",
        "You need to loop through this list and query the /leaders endpoint for each one. Save each `json` result in a dictionary called `leaders_per_country`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H8EdK2S9rvPJ"
      },
      "outputs": [],
      "source": [
        "# 4 lines\n",
        "# Initialize the leaders_per_country dictionary\n",
        "leaders_per_country = {}\n",
        "\n",
        "# Loop through countries, query the /leaders endpoint with cookies and parameters, and store results\n",
        "for country in countries:\n",
        "    leaders_per_country[country] = requests.get(leaders_url, cookies=cookies, params={\"country\": country}).json()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dictionary of all leaders per country in one line (1 line)\n",
        "leaders_per_country = {country: requests.get(leaders_url, cookies=cookies, params={\"country\": country}).json() for country in countries}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsA2j7s_RMgy"
      },
      "source": [
        "It is finally time to create a `get_leaders()` function for the above code. You will build on it later-on. This function takes no parameter. Inside it, you will need to:\n",
        "1. define the urls\n",
        "2. get the cookies\n",
        "2. get the countries\n",
        "3. loop over them and save their leaders in a dictionary\n",
        "4. return the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p6C7BgWQMxV2"
      },
      "outputs": [],
      "source": [
        "# < 15 lines\n",
        "# Define the get_leaders() function\n",
        "def get_leaders():\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = f\"{root_url}/cookie\"\n",
        "    countries_url = f\"{root_url}/countries\"\n",
        "    leaders_url = f\"{root_url}/leaders\"\n",
        "\n",
        "    cookies = requests.get(cookie_url).cookies\n",
        "    countries = requests.get(countries_url, cookies=cookies).json()\n",
        "\n",
        "    leaders_per_country = {}\n",
        "    for country in countries:\n",
        "        leaders_per_country[country] = requests.get(leaders_url, cookies=cookies, params={\"country\": country}).json()\n",
        "\n",
        "    return leaders_per_country\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I76-InoKuuV8"
      },
      "source": [
        "Test your function, save the result in the `leaders_per_country` dictionary and check its ouput."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eXwd8o7Gu8yG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leterme\n"
          ]
        }
      ],
      "source": [
        "# 2 lines\n",
        "# Test the function and store the result\n",
        "leaders_per_country = get_leaders()\n",
        "print(leaders_per_country['be'][1]['last_name'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Fc1mHySn9g"
      },
      "source": [
        "## 2. Extracting data from Wikipedia\n",
        "\n",
        "Query one of the leaders' Wikipedia urls and display its `text` (not JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cEKKqyTHr3fD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-la\n",
            "https://en.wikipedia.org/wiki/Barack_Obama\n"
          ]
        }
      ],
      "source": [
        "# 3 lines\n",
        "# Select one leader's Wikipedia URL and assign it to the wiki_url variable\n",
        "wiki_url = leaders_per_country[\"us\"][1][\"wikipedia_url\"]\n",
        "\n",
        "# Query the Wikipedia page with a proper User-Agent header\n",
        "response = requests.get(wiki_url, headers={\"User-Agent\": \"Mozilla/5.0 (Wikipedia Scraper Project - Educational Use)\"})\n",
        "\n",
        "# Display the text content of the page\n",
        "print(response.text[:100])  # Display first 1000 chars to avoid flooding the notebook\n",
        "\n",
        "print(wiki_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsqjiTYr8sK"
      },
      "source": [
        "Ouch! You get the raw HTML code of the webpage. If you try to deal with it without tools, you will be there all night. Instead, use the [beautiful soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) *external* library. You will find more info about it [here](../../2.python/2.python_advanced/05.Scraping/1.beautifulsoup_basic.ipynb) and [here](../../2.python/2.python_advanced/05.Scraping/2.beautifulsoup_advanced.ipynb)\n",
        "\n",
        "Using the Quickstart section, start by importing the library and loading the output of your `get_text()` function.\n",
        "\n",
        "Use the `prettify()` function and print it to take a look. You will start the actual parsing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h79ahwJvr7p-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-la\n"
          ]
        }
      ],
      "source": [
        "# 3 lines\n",
        "# Load the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Use prettify() to visualize the structure\n",
        "print(soup.prettify()[:100])  # Limit output to 1000 chars for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsLjaig7_dY"
      },
      "source": [
        "That looks better but you need to extract the right part of the webpage: the text of the first paragraph.\n",
        "\n",
        "It is a bit tricky because Wikipedia pages slightly differ in structure from one language to the next. We cannot simply get the text for the first HTML paragraph.\n",
        "\n",
        "You will start by getting all the HTML paragraphs from the HTML source and saving them in the `paragraphs` variable.\n",
        "\n",
        "Use the documentation or google the appropriate keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vs8HeBx19oyC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 128 paragraphs\n",
            "[<p class=\"mw-empty-elt\">\n",
            "</p>]\n"
          ]
        }
      ],
      "source": [
        "# 2 lines\n",
        "# Extract all HTML paragraphs and store them in the paragraphs variable\n",
        "paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "# Display how many paragraphs were found\n",
        "print(f\"Found {len(paragraphs)} paragraphs\")\n",
        "\n",
        "# Show the first few paragraphs to inspect\n",
        "print(paragraphs[:1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tlaL3aM9zoo"
      },
      "source": [
        "If you try different urls, you might find that the paragraph you want may be at a different index each time.\n",
        "\n",
        "That is where you need to be clever and ask yourself what would be a reliable way to identify the right index ie. which string matches only the first paragraph whatever the language...\n",
        "\n",
        "Spend a good 30 minutes on the problem and brainstorm with your fellow learners. If you come out empty handed, ask your coach.\n",
        "\n",
        "1. Loop over the HTML paragraphs\n",
        "2. When you have identified the correct one:\n",
        "   * Store the [text](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#output) inside the `first_paragraph` variable\n",
        "   * Exit the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0DduDXaQALau"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Hussein Obama II[a](born August 4, 1961) is an American politician who served as the 44thpresident of the United Statesfrom 2009 to 2017. A member of theDemocratic Party, he was the firstAfrican Americanpresident. Obama previously served as aU.S. senatorrepresenting Illinois from 2005 to 2008 and as anIllinois state senatorfrom 1997 to 2004.\n"
          ]
        }
      ],
      "source": [
        "# 1) Loop over the HTML paragraphs\n",
        "paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "first_paragraph = None\n",
        "for p in paragraphs:\n",
        "    text = p.get_text(strip=True)\n",
        "    # Heuristic: skip empty or very short paragraphs (nav/meta/fillers)\n",
        "    if len(text) > 60:\n",
        "        # 2) Identified the correct one\n",
        "        first_paragraph = text\n",
        "        #    * Store in first_paragraph\n",
        "        #    * Exit the loop\n",
        "        break \n",
        "\n",
        "# Optional: sanity check\n",
        "if first_paragraph is None:\n",
        "    print(\"No meaningful paragraph found.\")\n",
        "else:\n",
        "    print(first_paragraph)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFe-1LlIHBGm"
      },
      "source": [
        "At this stage, you can create a function to maintain consistency in your code. We will give you its *skeleton*, you will copy the code you wrote and make it work inside a function.\n",
        "\n",
        "Don't forget to test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wQORoweDHARO"
      },
      "outputs": [],
      "source": [
        "def get_first_paragraph(url):\n",
        "    \"\"\"Fetch a Wikipedia page and return the first meaningful intro paragraph.\"\"\"\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    first_paragraph = None\n",
        "    for p in soup.find_all(\"p\"):\n",
        "        text = p.get_text(\" \", strip=True)  # add a space between tags\n",
        "        if len(text) > 60:\n",
        "            first_paragraph = text\n",
        "            break\n",
        "\n",
        "    return first_paragraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "George Washington (February 22, 1732 [ O.S. February 11, 1731] [ a ] – December 14 , 1799) was a Founding Father and the first president of the United States , serving from 1789 to 1797. As commander of the Continental Army , Washington led Patriot forces to victory in the American Revolutionary War against the British Empire . He is commonly known as the Father of the Nation for his role in bringing about American independence .\n"
          ]
        }
      ],
      "source": [
        "# Test: 3 lines\n",
        "test_url = leaders_per_country[\"us\"][0][\"wikipedia_url\"]\n",
        "print(get_first_paragraph(test_url))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtoM4dgsBVoD"
      },
      "source": [
        "### 2a. Regular expressions to the rescue\n",
        "\n",
        "Now that you have extracted the content of the first paragraph, the only thing that remains to finish your Wikipedia scraper is to sanitize the output.\n",
        "\n",
        "Indeed some Wikipedia references, HTML code, phonetic pronunciation etc. may linger. You might find *regular expressions* handy to get rid of them and obtain pristine text. You will find some useful documentation about regular expressions [here](../../2.python/2.python_advanced/03.Regex/regex.ipynb)\n",
        "\n",
        "Once you have one of your regex working online, try it in the cell below. \n",
        "\n",
        "Hints: \n",
        "* Check the `sub()` method documentation.\n",
        "* Make sure to test urls in different languages. Some may look good but other do not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "محمد الخامس بن يوسف بن الحسن بن محمد بن عبد الرحمن بن هشام بن محمد بن عبد الله بن إسماعيل بن الشريف بن علي العلوي وُلد ( 1327 هـ / 10 أغسطس 1909م بالقصر السلطاني بفاس ) [ 1 ] وتوفي ( 1381 هـ / 26 فبراير 1961م بالرباط ) خَلَف والده السلطان مولاي يوسف الذي توفي بُكرة يوم الخميس 22 جمادى الأولى سنة 1346 هـ موافق 17 نوفمبر سنة 1927م [ 2 ] فبويع ابنه سيدي محمد سلطانا للمغرب في اليوم الموالي بعد صلاة الجمعة 23 جمادى الأولى سنة 1346 هـ موافق 18 نوفمبر سنة 1927م في القصر السلطاني بفاس [ 3 ] ولم يزل سلطان المغرب إلى سنة 1957م ، قضى منها المنفى بين ( 1953 - 1955 )، ثم اتخذ لقب الملك سنة 1957م ولم يزل ملكا إلى وفاته سنة 1961م ، ساند السلطان محمد الخامس نضالات الحركة الوطنية المغربية المطالبة بتحقيق الاستقلال ، الشيء الذي دفعه إلى الاصطدام بسلطات الحماية . وكانت النتيجة قيام سلطات الحماية بنفيه إلى مدغشقر . وعلى إثر ذلك اندلعت مظاهرات مطالبة بعودته إلى وطنه. وأمام اشتداد حدة المظاهرات، قبلت السلطات الفرنسية بإرجاع السلطان إلى عرشه يوم 16 نوفمبر 1955 . وبعد بضعة شهور تم إعلان استقلال المغرب . كان الملك محمد الخامس يكنى: أبا عبد الله. [ 1 ] [ 4 ] [ 5 ]\n"
          ]
        }
      ],
      "source": [
        "test_url = leaders_per_country[\"ma\"][2][\"wikipedia_url\"]\n",
        "print(get_first_paragraph(test_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7DHEAb6oBUxd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "محمد الخامس بن يوسف بن الحسن بن محمد بن عبد الرحمن بن هشام بن محمد بن عبد الله بن إسماعيل بن الشريف بن علي العلوي وُلد ( 1327 هـ / 10 أغسطس 1909م بالقصر السلطاني بفاس )  وتوفي ( 1381 هـ / 26 فبراير 1961م بالرباط ) خَلَف والده السلطان مولاي يوسف الذي توفي بُكرة يوم الخميس 22 جمادى الأولى سنة 1346 هـ موافق 17 نوفمبر سنة 1927م  فبويع ابنه سيدي محمد سلطانا للمغرب في اليوم الموالي بعد صلاة الجمعة 23 جمادى الأولى سنة 1346 هـ موافق 18 نوفمبر سنة 1927م في القصر السلطاني بفاس  ولم يزل سلطان المغرب إلى سنة 1957م ، قضى منها المنفى بين ( 1953 - 1955 )، ثم اتخذ لقب الملك سنة 1957م ولم يزل ملكا إلى وفاته سنة 1961م ، ساند السلطان محمد الخامس نضالات الحركة الوطنية المغربية المطالبة بتحقيق الاستقلال ، الشيء الذي دفعه إلى الاصطدام بسلطات الحماية . وكانت النتيجة قيام سلطات الحماية بنفيه إلى مدغشقر . وعلى إثر ذلك اندلعت مظاهرات مطالبة بعودته إلى وطنه. وأمام اشتداد حدة المظاهرات، قبلت السلطات الفرنسية بإرجاع السلطان إلى عرشه يوم 16 نوفمبر 1955 . وبعد بضعة شهور تم إعلان استقلال المغرب . كان الملك محمد الخامس يكنى: أبا عبد الله.\n"
          ]
        }
      ],
      "source": [
        "# 3 lines\n",
        "\n",
        "# clean the paragraph by removing reference markers and brackets\n",
        "clean_paragraph = re.sub(r'\\[[^\\]]*\\]', '', get_first_paragraph(test_url))\n",
        "\n",
        "# display the cleaned text\n",
        "print(clean_paragraph.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QekqWs-4E0bK"
      },
      "source": [
        "Overwrite the `get_first_paragraph()` function by applying your regex to the first paragraph before returning it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "voT-jzd7FMOc"
      },
      "outputs": [],
      "source": [
        "# 10 lines\n",
        "\n",
        "def get_first_paragraph(url):\n",
        "    \"\"\"Fetch a Wikipedia page, extract and clean the first meaningful intro paragraph.\"\"\"\n",
        "    \n",
        "    # query the Wikipedia URL\n",
        "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    \n",
        "    # parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    \n",
        "    # initialize variable\n",
        "    first_paragraph = None\n",
        "    \n",
        "    # loop over HTML paragraphs\n",
        "    for p in soup.find_all(\"p\"):\n",
        "        text = p.get_text(\" \", strip=True)\n",
        "        if len(text) > 60:\n",
        "            first_paragraph = text\n",
        "            break\n",
        "\n",
        "    if not first_paragraph:\n",
        "        return None\n",
        "\n",
        "    # --- Apply regex cleaning before returning ---\n",
        "    # remove reference markers like [1], [a], etc.\n",
        "    cleaned = re.sub(r'\\[[^\\]]*\\]', '', first_paragraph)\n",
        "    # remove phonetic pronunciations like /ˈwɒʃɪŋtən/\n",
        "    cleaned = re.sub(r'/[^/]+/', '', cleaned)\n",
        "    # collapse extra spaces\n",
        "    cleaned = re.sub(r'\\s{2,}', ' ', cleaned).strip()\n",
        "    \n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "محمد الخامس بن يوسف بن الحسن بن محمد بن عبد الرحمن بن هشام بن محمد بن عبد الله بن إسماعيل بن الشريف بن علي العلوي وُلد ( 1327 هـ 26 فبراير 1961م بالرباط ) خَلَف والده السلطان مولاي يوسف الذي توفي بُكرة يوم الخميس 22 جمادى الأولى سنة 1346 هـ موافق 17 نوفمبر سنة 1927م فبويع ابنه سيدي محمد سلطانا للمغرب في اليوم الموالي بعد صلاة الجمعة 23 جمادى الأولى سنة 1346 هـ موافق 18 نوفمبر سنة 1927م في القصر السلطاني بفاس ولم يزل سلطان المغرب إلى سنة 1957م ، قضى منها المنفى بين ( 1953 - 1955 )، ثم اتخذ لقب الملك سنة 1957م ولم يزل ملكا إلى وفاته سنة 1961م ، ساند السلطان محمد الخامس نضالات الحركة الوطنية المغربية المطالبة بتحقيق الاستقلال ، الشيء الذي دفعه إلى الاصطدام بسلطات الحماية . وكانت النتيجة قيام سلطات الحماية بنفيه إلى مدغشقر . وعلى إثر ذلك اندلعت مظاهرات مطالبة بعودته إلى وطنه. وأمام اشتداد حدة المظاهرات، قبلت السلطات الفرنسية بإرجاع السلطان إلى عرشه يوم 16 نوفمبر 1955 . وبعد بضعة شهور تم إعلان استقلال المغرب . كان الملك محمد الخامس يكنى: أبا عبد الله.\n"
          ]
        }
      ],
      "source": [
        "test_url = leaders_per_country[\"ma\"][2][\"wikipedia_url\"]\n",
        "print(get_first_paragraph(test_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY7bM7Z1Nr0K"
      },
      "source": [
        "## 3. Putting it all together\n",
        "\n",
        "Let's go back to your `get_leaders()` function and update it with an *inner* loop over each leader. You will query the url provided and extract the first paragraph using the `get_first_paragraph()` function you just finished. You will then update that `leader`'s dictionary and move on to the next one.\n",
        "\n",
        "Notice, the rest of the code should not change since you modify the leader's data one by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# < 20 lines\n",
        "\n",
        "def get_leaders():\n",
        "    \"\"\"Retrieve leaders for all countries and enrich them with Wikipedia intro paragraphs.\"\"\"\n",
        "\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = f\"{root_url}/cookie\"\n",
        "    countries_url = f\"{root_url}/countries\"\n",
        "    leaders_url = f\"{root_url}/leaders\"\n",
        "\n",
        "    # Get cookie\n",
        "    cookies = requests.get(cookie_url).cookies\n",
        "    # Get countries\n",
        "    countries = requests.get(countries_url, cookies=cookies).json()\n",
        "\n",
        "    # Initialize container\n",
        "    leaders_per_country = {}\n",
        "    # Loop through each country\n",
        "    for country in countries:\n",
        "        leaders = requests.get(leaders_url, cookies=cookies, params={\"country\": country}).json()\n",
        "\n",
        "        # Inner loop: scrape first paragraph for each leader\n",
        "        for leader in leaders:\n",
        "            wiki_url = leader.get(\"wikipedia_url\")\n",
        "            leader[\"first_paragraph\"] = get_first_paragraph(wiki_url) if wiki_url else None\n",
        "\n",
        "        # Save to main dictionary\n",
        "        leaders_per_country[country] = leaders\n",
        "\n",
        "    # Return results\n",
        "    return leaders_per_country\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RICG4T3DPZ1b"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check the output of your function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m leaders_per_country \u001b[38;5;241m=\u001b[39m \u001b[43mget_leaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(leaders_per_country[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
            "Cell \u001b[1;32mIn[24], line 24\u001b[0m, in \u001b[0;36mget_leaders\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Inner loop: scrape first paragraph for each leader\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m leader \u001b[38;5;129;01min\u001b[39;00m leaders:\n\u001b[1;32m---> 24\u001b[0m     wiki_url \u001b[38;5;241m=\u001b[39m \u001b[43mleader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia_url\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     leader[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_paragraph\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_first_paragraph(wiki_url) \u001b[38;5;28;01mif\u001b[39;00m wiki_url \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Save to main dictionary\u001b[39;00m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "# Check the output of your function\n",
        "leaders_per_country = get_leaders()\n",
        "print(leaders_per_country[\"us\"][0])\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8sCKxGrnJCxv"
      },
      "source": [
        "Does the function crash in the middle of the loop? Chances are the cookies have expired while looping over the leaders.\n",
        "\n",
        "Modify your function with an *exception* or check if the `status_code` is a cookie error. In either case, get new cookies and query the api again.\n",
        "\n",
        "If your code did not crash,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rgPd2dxgJiW1"
      },
      "outputs": [],
      "source": [
        "# < 25 lines\n",
        "\n",
        "def get_leaders(country):\n",
        "    \"\"\"Retrieve leaders for a specific country and enrich them with Wikipedia intro paragraphs.\"\"\"\n",
        "    \n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = f\"{root_url}/cookie\"\n",
        "    leaders_url = f\"{root_url}/leaders\"\n",
        "    \n",
        "    # Helper to get a fresh cookie\n",
        "    def get_cookie():\n",
        "        return requests.get(cookie_url).cookies\n",
        "    \n",
        "    cookies = get_cookie()\n",
        "    leaders_per_country = {}\n",
        "\n",
        "    try:\n",
        "        # Request leaders for the given country\n",
        "        resp = requests.get(leaders_url, cookies=cookies, params={\"country\": country})\n",
        "\n",
        "        # If cookie expired or invalid, refresh and retry\n",
        "        if resp.status_code != 200 or \"cookie\" in resp.text.lower():\n",
        "            print(f\"Cookie expired while fetching {country}. Refreshing...\")\n",
        "            cookies = get_cookie()\n",
        "            resp = requests.get(leaders_url, cookies=cookies, params={\"country\": country})\n",
        "\n",
        "        leaders = resp.json()\n",
        "\n",
        "        # Enrich each leader with their Wikipedia first paragraph\n",
        "        for leader in leaders:\n",
        "            wiki_url = leader.get(\"wikipedia_url\")\n",
        "            leader[\"first_paragraph\"] = get_first_paragraph(wiki_url) if wiki_url else None\n",
        "\n",
        "        leaders_per_country[country] = leaders\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {country}: {e}\")\n",
        "    \n",
        "    return leaders_per_country\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JvV-kPsKLl0"
      },
      "source": [
        "Check the output of your function again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mPXT-cxgKQof"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'Q23', 'first_name': 'George', 'last_name': 'Washington', 'birth_date': '1732-02-22', 'death_date': '1799-12-14', 'place_of_birth': 'Westmoreland County', 'wikipedia_url': 'https://en.wikipedia.org/wiki/George_Washington', 'start_mandate': '1789-04-30', 'end_mandate': '1797-03-04', 'first_paragraph': 'George Washington (February 22, 1732 – December 14 , 1799) was a Founding Father and the first president of the United States , serving from 1789 to 1797. As commander of the Continental Army , Washington led Patriot forces to victory in the American Revolutionary War against the British Empire . He is commonly known as the Father of the Nation for his role in bringing about American independence .'}\n"
          ]
        }
      ],
      "source": [
        "# Check the output of your function (2 lines)\n",
        "leaders_us = get_leaders(\"us\")\n",
        "print(leaders_us[\"us\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9lgxh_NMSO3"
      },
      "source": [
        "Well done! It took a while however... Let's speed things up. The main *bottleneck* is the loop. We call on the Wikipedia website many times.\n",
        "\n",
        "You will use the same *session* to call all the wikipedia pages. Check the *Advanced Usage* section of the Requests module's documentation.\n",
        "\n",
        "Start by modifying the `get_first_paragraph()` function to accept a session parameter and adjust the `get()` method call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XTEVLIJ-V7z1"
      },
      "outputs": [],
      "source": [
        "# < 20 lines\n",
        "def get_first_paragraph(url, session=None):\n",
        "    \"\"\"Fetch and clean the first paragraph of a Wikipedia page using a shared session.\"\"\"\n",
        "    \n",
        "    # use provided session or create a temporary one\n",
        "    session = session or requests.Session()\n",
        "    \n",
        "    # query the Wikipedia URL (using session)\n",
        "    response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    \n",
        "    # parse HTML\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    \n",
        "    # extract the first meaningful paragraph\n",
        "    first_paragraph = None\n",
        "    for p in soup.find_all(\"p\"):\n",
        "        text = p.get_text(\" \", strip=True)\n",
        "        if len(text) > 60:\n",
        "            first_paragraph = text\n",
        "            break\n",
        "    \n",
        "    if not first_paragraph:\n",
        "        return None\n",
        "\n",
        "    # clean the text with regex\n",
        "    cleaned = re.sub(r'\\[[^\\]]*\\]', '', first_paragraph)   # remove [1], [a], etc.\n",
        "    cleaned = re.sub(r'/[^/]+/', '', cleaned)              # remove phonetic /ˈ.../\n",
        "    cleaned = re.sub(r'\\s{2,}', ' ', cleaned).strip()      # collapse spaces\n",
        "\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "George Washington (February 22, 1732 – December 14 , 1799) was a Founding Father and the first president of the United States , serving from 1789 to 1797. As commander of the Continental Army , Washington led Patriot forces to victory in the American Revolutionary War against the British Empire . He is commonly known as the Father of the Nation for his role in bringing about American independence .\n"
          ]
        }
      ],
      "source": [
        "with requests.Session() as s:\n",
        "    test_url = \"https://en.wikipedia.org/wiki/George_Washington\"\n",
        "    print(get_first_paragraph(test_url, s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gcgHMkeWK5-"
      },
      "source": [
        "Modify your `get_leaders()` function to make use of a single session for all the Wikipedia calls.\n",
        "1. create a `Session` object outside of the loop over countries.\n",
        "2. pass it to the `get_first_paragraph()` function as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Kt6JtKDpOAIe"
      },
      "outputs": [],
      "source": [
        "# < 25 lines\n",
        "def get_leaders(country):\n",
        "    \"\"\"Retrieve leaders for a specific country and enrich them with Wikipedia intro paragraphs.\"\"\"\n",
        "    \n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = f\"{root_url}/cookie\"\n",
        "    countries_url = f\"{root_url}/countries\"\n",
        "    leaders_url = f\"{root_url}/leaders\"\n",
        "    \n",
        "    # Helper to get a fresh cookie\n",
        "    def get_cookie():\n",
        "        return requests.get(cookie_url).cookies\n",
        "    \n",
        "    cookies = requests.get(cookie_url).cookies\n",
        "    countries = requests.get(countries_url, cookies=cookies).json()\n",
        "\n",
        "    leaders_per_country = {}\n",
        "\n",
        "    try:\n",
        "        # Request leaders for the given country\n",
        "        resp = requests.get(leaders_url, cookies=cookies, params={\"country\": country})\n",
        "\n",
        "        # If cookie expired or invalid, refresh and retry\n",
        "        if resp.status_code != 200 or \"cookie\" in resp.text.lower():\n",
        "            print(f\"Cookie expired while fetching {country}. Refreshing...\")\n",
        "            cookies = get_cookie()\n",
        "            resp = requests.get(leaders_url, cookies=cookies, params={\"country\": country})\n",
        "\n",
        "        leaders = resp.json()\n",
        "\n",
        "        # Enrich each leader with their Wikipedia first paragraph\n",
        "        for leader in leaders:\n",
        "            wiki_url = leader.get(\"wikipedia_url\")\n",
        "            leader[\"first_paragraph\"] = get_first_paragraph(wiki_url) if wiki_url else None\n",
        "\n",
        "        leaders_per_country[country] = leaders\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {country}: {e}\")\n",
        "    \n",
        "    # Create a single session for ALL Wikipedia calls and return it\n",
        "    wiki_session = requests.Session()\n",
        "    return leaders_per_country, wiki_session\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leader name: Mohammed None\n",
            "Wikipedia URL: https://ar.wikipedia.org/wiki/%D9%85%D8%AD%D9%85%D8%AF_%D8%A7%D9%84%D8%B3%D8%A7%D8%AF%D8%B3_%D8%A8%D9%86_%D8%A7%D9%84%D8%AD%D8%B3%D9%86\n",
            "\n",
            "Extracted first paragraph:\n",
            "\n",
            "مُحمد السادس بن الحسن الثاني العلوي (مواليد 21 أغسطس 1963) هو ملك المملكة المغربية منذ عام 1999 والملك الثالث والعشرون للمغرب من سلالة العلويين الفيلاليين ، تولى الحكم خلفًا لوالده الملك الحسن الثاني بعد وفاته، وبويع ملكًا يوم الجمعة 9 ربيع الثاني سنة 1420 هـ الموافق 23 يوليو 1999 بالقصر الملكي بالرباط .\n"
          ]
        }
      ],
      "source": [
        "# Call get_leaders() — this now returns both the leaders data and the wiki session\n",
        "leaders_per_country, wiki_session = get_leaders(\"ma\")\n",
        "\n",
        "# Select the first Moroccan leader for example (country code \"ma\")\n",
        "leader = leaders_per_country[\"ma\"][0]\n",
        "url = leader.get(\"wikipedia_url\")\n",
        "\n",
        "# Get the first paragraph using the same session\n",
        "leader[\"first_paragraph\"] = get_first_paragraph(url, session=wiki_session) if url else None\n",
        "\n",
        "# Print the results\n",
        "print(f\"Leader name: {leader['first_name']} {leader['last_name']}\")\n",
        "print(f\"Wikipedia URL: {url}\")\n",
        "print(\"\\nExtracted first paragraph:\\n\")\n",
        "print(leader[\"first_paragraph\"])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LtLTvw3wPqe0"
      },
      "source": [
        "## 4. Saving your hard work\n",
        "\n",
        "The final step is to save the ``leaders_per_country`` dictionary in the `leaders.json` file using the [json](https://docs.python.org/3/library/json.html) module. Check out the `with` statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pTNGKKrOjNDk"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(leaders_per_country, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Save as CSV (1 line)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[0;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m: c, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ml} \u001b[38;5;28;01mfor\u001b[39;00m c, leaders \u001b[38;5;129;01min\u001b[39;00m leaders_per_country\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m leaders\n\u001b[0;32m      8\u001b[0m ])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleaders.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Save as JSON (2 lines)\n",
        "with open(\"leaders.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(leaders_per_country, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Save as CSV (1 line)\n",
        "pd.DataFrame([\n",
        "    {\"country\": c, **l} for c, leaders in leaders_per_country.items() for l in leaders\n",
        "]).to_csv(\"leaders.csv\", index=False, encoding=\"utf-8\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v7uf_kfGCWmM"
      },
      "source": [
        "Make sure the file can be read back. Write the code to read the file. And check the variables are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "4VwNjBYyjPzs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Same content: True\n"
          ]
        }
      ],
      "source": [
        "# 3 lines\n",
        "# Read back the JSON file (1 line)\n",
        "with open(\"leaders.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    leaders_check = json.load(f)\n",
        "\n",
        "# Compare variables (1 line)\n",
        "print(\"Same content:\", leaders_per_country.keys() == leaders_check.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>id</th>\n",
              "      <th>first_name</th>\n",
              "      <th>last_name</th>\n",
              "      <th>birth_date</th>\n",
              "      <th>death_date</th>\n",
              "      <th>place_of_birth</th>\n",
              "      <th>wikipedia_url</th>\n",
              "      <th>start_mandate</th>\n",
              "      <th>end_mandate</th>\n",
              "      <th>first_paragraph</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>be</td>\n",
              "      <td>Q12978</td>\n",
              "      <td>Guy</td>\n",
              "      <td>Verhofstadt</td>\n",
              "      <td>1953-04-11</td>\n",
              "      <td>None</td>\n",
              "      <td>Dendermonde</td>\n",
              "      <td>https://nl.wikipedia.org/wiki/Guy_Verhofstadt</td>\n",
              "      <td>1999-07-12</td>\n",
              "      <td>2008-03-20</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>be</td>\n",
              "      <td>Q12981</td>\n",
              "      <td>Yves</td>\n",
              "      <td>Leterme</td>\n",
              "      <td>1960-10-06</td>\n",
              "      <td>None</td>\n",
              "      <td>Wervik</td>\n",
              "      <td>https://nl.wikipedia.org/wiki/Yves_Leterme</td>\n",
              "      <td>2009-11-25</td>\n",
              "      <td>2011-12-06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>be</td>\n",
              "      <td>Q12983</td>\n",
              "      <td>Herman</td>\n",
              "      <td>None</td>\n",
              "      <td>1947-10-31</td>\n",
              "      <td>None</td>\n",
              "      <td>Etterbeek</td>\n",
              "      <td>https://nl.wikipedia.org/wiki/Herman_Van_Rompuy</td>\n",
              "      <td>2008-12-30</td>\n",
              "      <td>2009-11-25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>be</td>\n",
              "      <td>Q14989</td>\n",
              "      <td>Léon</td>\n",
              "      <td>Delacroix</td>\n",
              "      <td>1867-12-27</td>\n",
              "      <td>1929-10-15</td>\n",
              "      <td>Saint-Josse-ten-Noode</td>\n",
              "      <td>https://nl.wikipedia.org/wiki/L%C3%A9on_Delacroix</td>\n",
              "      <td>1918-11-21</td>\n",
              "      <td>1920-11-20</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>be</td>\n",
              "      <td>Q14990</td>\n",
              "      <td>Henry</td>\n",
              "      <td>Carton</td>\n",
              "      <td>1869-01-31</td>\n",
              "      <td>1951-05-06</td>\n",
              "      <td>Brussels</td>\n",
              "      <td>https://nl.wikipedia.org/wiki/Henri_Carton_de_...</td>\n",
              "      <td>1920-11-20</td>\n",
              "      <td>1921-12-16</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  country      id first_name    last_name  birth_date  death_date  \\\n",
              "0      be  Q12978        Guy  Verhofstadt  1953-04-11        None   \n",
              "1      be  Q12981       Yves      Leterme  1960-10-06        None   \n",
              "2      be  Q12983     Herman         None  1947-10-31        None   \n",
              "3      be  Q14989       Léon    Delacroix  1867-12-27  1929-10-15   \n",
              "4      be  Q14990      Henry       Carton  1869-01-31  1951-05-06   \n",
              "\n",
              "          place_of_birth                                      wikipedia_url  \\\n",
              "0            Dendermonde      https://nl.wikipedia.org/wiki/Guy_Verhofstadt   \n",
              "1                 Wervik         https://nl.wikipedia.org/wiki/Yves_Leterme   \n",
              "2              Etterbeek    https://nl.wikipedia.org/wiki/Herman_Van_Rompuy   \n",
              "3  Saint-Josse-ten-Noode  https://nl.wikipedia.org/wiki/L%C3%A9on_Delacroix   \n",
              "4               Brussels  https://nl.wikipedia.org/wiki/Henri_Carton_de_...   \n",
              "\n",
              "  start_mandate end_mandate first_paragraph  \n",
              "0    1999-07-12  2008-03-20             NaN  \n",
              "1    2009-11-25  2011-12-06             NaN  \n",
              "2    2008-12-30  2009-11-25             NaN  \n",
              "3    1918-11-21  1920-11-20             NaN  \n",
              "4    1920-11-20  1921-12-16             NaN  "
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read JSON file (1 line)\n",
        "with open(\"leaders.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert nested JSON into a flat DataFrame (1 line)\n",
        "leaders_df = pd.DataFrame([\n",
        "    {\"country\": c, **l} for c, leaders in data.items() for l in leaders\n",
        "])\n",
        "\n",
        "# Display first rows (1 line)\n",
        "leaders_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fW_U7gXktyv"
      },
      "source": [
        "Make a function `save(leaders_per_country)` to call this code easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfknpnTljqUd"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "def save(leaders_per_country):\n",
        "    \"\"\"Save leaders data to both JSON and CSV files.\"\"\"\n",
        "    # Save as JSON \n",
        "    with open(\"leaders.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(leaders_per_country, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Save as CSV\n",
        "    pd.DataFrame([\n",
        "        {\"country\": c, **l} for c, leaders in leaders_per_country.items() for l in leaders\n",
        "    ]).to_csv(\"leaders.csv\", index=False, encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lWQ6bbn31cix"
      },
      "outputs": [],
      "source": [
        "# Call the function (1 line)\n",
        "save(leaders_per_country)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5W6nPihlQo9"
      },
      "source": [
        "## 5. Tidy things up in a stand-alone python script\n",
        "\n",
        "Congratulations! You now have a working scraper! However, your code is scattered throughout this notebook along side the tutorials. Hardly production ready...\n",
        "\n",
        "Copy and paste what you need in a separate `leaders_scraper.py` file.\n",
        "Make sure it works by calling `python3 leaders_scraper.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0cvv193mlxY"
      },
      "source": [
        "## (Optional) To go further\n",
        "\n",
        "If you want to practice scraping, you can read this section and tackle the exercises.\n",
        "\n",
        "1. Restructure your code by using OOP (see ReadMe).\n",
        "2. You have noticed the API returns very partial results for country leaders. Many are missing. Overwrite the `get_leaders()` function to get its list from Wikipedia and extract their *personal details* from the frame on the side.\n",
        "\n",
        "Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
